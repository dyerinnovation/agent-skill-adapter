apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "tgi.fullname" . }}
  labels:
    {{- include "tgi.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      {{- include "tgi.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "tgi.selectorLabels" . | nindent 8 }}
    spec:
      containers:
      - name: vllm
        image: {{ .Values.image.repository }}:{{ .Values.image.tag }}
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
        args:
        - --model
        - {{ .Values.model.id | quote }}
        - --max-model-len
        - {{ .Values.model.maxModelLen | quote }}
        - --port
        - "8080"
        - --enable-reasoning
        - --reasoning-parser
        - deepseek_r1
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP
        env:
        - name: HF_HOME
          value: /data
        - name: HUGGING_FACE_HUB_TOKEN
          value: ""
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 120
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 6
        readinessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 60
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 6
        resources:
          {{- toYaml .Values.resources | nindent 10 }}
        volumeMounts:
        - name: hf-cache
          mountPath: /data
        - name: shm
          mountPath: /dev/shm
      volumes:
      - name: hf-cache
        hostPath:
          path: {{ .Values.persistence.hostPath }}
          type: DirectoryOrCreate
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 16Gi
